{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\!Study\\Other\\DataScience_TestTask\\ner\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "label_list = [\"O\", \"B-MOUNT\", \"I-MOUNT\"]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "# Convert labels to IDs\n",
    "def encode_labels(labels):\n",
    "    ids = [label_to_id[label] for label in labels]\n",
    "    padding_length = tokenizer.model_max_length - len(ids)\n",
    "    ids += [label_list.index('O')] * padding_length\n",
    "    return ids\n",
    "\n",
    "def encode_tokens(tokens):\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    padding_length = tokenizer.model_max_length - len(ids)\n",
    "    ids += [tokenizer.pad_token_id] * padding_length\n",
    "    return ids\n",
    "\n",
    "def prepare_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Function to read the json file with data and convert\n",
    "    it to a Dataset object with encoded labels and tokens\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "    dataset = Dataset.from_list(dataset)\n",
    "    dataset = dataset.map(lambda x: {'labels': encode_labels(x['labels']),\n",
    "                                     'input_ids': encode_tokens(x['tokens'])})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3064.36 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2857.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = prepare_dataset(\"train_dataset.json\")\n",
    "val_dataset = prepare_dataset(\"val_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at dslim/distilbert-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\", num_labels=len(label_list), ignore_mismatched_sizes=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 63/630 [06:56<48:10,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.002114757662639022, 'eval_runtime': 2.8179, 'eval_samples_per_second': 35.488, 'eval_steps_per_second': 2.484, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 126/630 [13:53<42:40,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0008995987591333687, 'eval_runtime': 2.7405, 'eval_samples_per_second': 36.49, 'eval_steps_per_second': 2.554, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 30%|███       | 189/630 [20:50<37:16,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007847411907278001, 'eval_runtime': 2.7604, 'eval_samples_per_second': 36.227, 'eval_steps_per_second': 2.536, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|████      | 252/630 [27:47<31:59,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007230451446957886, 'eval_runtime': 2.7648, 'eval_samples_per_second': 36.168, 'eval_steps_per_second': 2.532, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|█████     | 315/630 [34:44<26:40,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.000689782144036144, 'eval_runtime': 2.7596, 'eval_samples_per_second': 36.237, 'eval_steps_per_second': 2.537, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 378/630 [41:41<21:20,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.000546968134585768, 'eval_runtime': 2.7599, 'eval_samples_per_second': 36.233, 'eval_steps_per_second': 2.536, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|███████   | 441/630 [48:38<16:01,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0008653226541355252, 'eval_runtime': 2.7623, 'eval_samples_per_second': 36.202, 'eval_steps_per_second': 2.534, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 500/630 [55:11<14:25,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0048, 'grad_norm': 0.0009105164790526032, 'learning_rate': 4.126984126984127e-06, 'epoch': 7.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 504/630 [55:36<10:51,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0006288138101808727, 'eval_runtime': 2.7556, 'eval_samples_per_second': 36.289, 'eval_steps_per_second': 2.54, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 567/630 [1:02:33<05:19,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0006100510363467038, 'eval_runtime': 2.7601, 'eval_samples_per_second': 36.231, 'eval_steps_per_second': 2.536, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 630/630 [1:09:31<00:00,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.000589806295465678, 'eval_runtime': 2.5816, 'eval_samples_per_second': 38.735, 'eval_steps_per_second': 2.711, 'epoch': 10.0}\n",
      "{'train_runtime': 4171.1197, 'train_samples_per_second': 2.397, 'train_steps_per_second': 0.151, 'train_loss': 0.003826779875135611, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=630, training_loss=0.003826779875135611, metrics={'train_runtime': 4171.1197, 'train_samples_per_second': 2.397, 'train_steps_per_second': 0.151, 'total_flos': 1306554624000000.0, 'train_loss': 0.003826779875135611, 'epoch': 10.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./distilbert-ner-tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(predictions, id_to_label):\n",
    "    predicted_labels = []\n",
    "    for pred in predictions:\n",
    "        label_ids = [id_to_label[label_id] for label_id in pred]\n",
    "        predicted_labels.append(label_ids)\n",
    "    return predicted_labels\n",
    "\n",
    "def run_inference(sample):\n",
    "    tokenized_sentence = tokenizer.tokenize(sample)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "    model_input = Dataset.from_list([{'input_ids': ids}])\n",
    "    predictions = trainer.predict(model_input)\n",
    "    preds = predictions.predictions.argmax(-1)\n",
    "    decoded_predictions = decode_predictions(preds, id_to_label)\n",
    "    return tokenized_sentence, decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"The highest mountain on Earth is Mount Everest in the Himalayas of Asia, whose summit is 8,850 m (29,035 ft) above mean sea level.\n",
    "                    The highest known mountain on any planet in the Solar System is Olympus Mons on Mars at 21,171 m (69,459 ft).\n",
    "                    The tallest mountain including submarine terrain is Mauna Kea in Hawaii from its underwater base at 9,330 m (30,610 ft)\n",
    "                    and some scientists consider it to be the tallest on earth.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'highest', 'mountain', 'on', 'Earth', 'is', 'Mount', 'Everest', 'in', 'the', 'Him', '##alaya', '##s', 'of', 'Asia', ',', 'whose', 'summit', 'is', '8', ',', '850', 'm', '(', '29', ',', '03', '##5', 'ft', ')', 'above', 'mean', 'sea', 'level', '.', 'The', 'highest', 'known', 'mountain', 'on', 'any', 'planet', 'in', 'the', 'Solar', 'System', 'is', 'O', '##ly', '##mpus', 'Mon', '##s', 'on', 'Mars', 'at', '21', ',', '171', 'm', '(', '69', ',', '45', '##9', 'ft', ')', '.', 'The', 'tallest', 'mountain', 'including', 'submarine', 'terrain', 'is', 'Ma', '##una', 'Ke', '##a', 'in', 'Hawaii', 'from', 'its', 'underwater', 'base', 'at', '9', ',', '330', 'm', '(', '30', ',', '610', 'ft', ')', 'and', 'some', 'scientists', 'consider', 'it', 'to', 'be', 'the', 'tallest', 'on', 'earth', '.']\n",
      "Predicted labels: [['O', 'O', 'O', 'O', 'O', 'O', 'B-MOUNT', 'I-MOUNT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MOUNT', 'I-MOUNT', 'I-MOUNT', 'I-MOUNT', 'I-MOUNT', 'I-MOUNT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MOUNT', 'I-MOUNT', 'I-MOUNT', 'I-MOUNT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence, decoded_predictions = run_inference(test_sentence)\n",
    "print(f\"Tokens: {tokenized_sentence}\")\n",
    "print(f\"Predicted labels: {decoded_predictions[0]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and Labels:\n",
      "\n",
      "Mount        -> B-MOUNT\n",
      "Everest      -> I-MOUNT\n",
      "\n",
      "O            -> B-MOUNT\n",
      "##ly         -> I-MOUNT\n",
      "##mpus       -> I-MOUNT\n",
      "Mon          -> I-MOUNT\n",
      "##s          -> I-MOUNT\n",
      "on           -> I-MOUNT\n",
      "\n",
      "Ma           -> B-MOUNT\n",
      "##una        -> I-MOUNT\n",
      "Ke           -> I-MOUNT\n",
      "##a          -> I-MOUNT\n",
      "\n",
      "Detected Entities:\n",
      "\n",
      "Entity: Mount Everest\n",
      "Entity: Olympus Mons on\n",
      "Entity: Mauna Kea\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(tokens, labels, entities):\n",
    "    # Print tokens with corresponding labels\n",
    "    print(\"Tokens and Labels:\")\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == \"B-MOUNT\":\n",
    "            print()\n",
    "            print(f\"{token:12} -> {label}\")\n",
    "        if label == \"I-MOUNT\":\n",
    "            print(f\"{token:12} -> {label}\")\n",
    "    \n",
    "    # Print the extracted entities\n",
    "    print(\"\\nDetected Entities:\\n\")\n",
    "    for entity in entities:\n",
    "        string = tokenizer.convert_tokens_to_string(entity.split(\" \"))\n",
    "        print(f\"Entity: {string}\")\n",
    "    \n",
    "def extract_entities(tokens, labels):\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == \"B-MOUNT\":  # Beginning of a new entity\n",
    "            if current_entity:  # Add the previous entity to the list if exists\n",
    "                entities.append(\" \".join(current_entity))\n",
    "            current_entity = [token]  # Start a new entity\n",
    "        elif label == \"I-MOUNT\":  # Continuation of the current entity\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity:  # Add the entity to the list if exists\n",
    "                entities.append(\" \".join(current_entity))\n",
    "                current_entity = []  # Reset the current entity\n",
    "    \n",
    "    # Catch any remaining entity at the end\n",
    "    if current_entity:\n",
    "        entities.append(\" \".join(current_entity))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "entities = extract_entities(tokenized_sentence, decoded_predictions[0])\n",
    "pretty_print(tokenized_sentence, decoded_predictions[0], entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
