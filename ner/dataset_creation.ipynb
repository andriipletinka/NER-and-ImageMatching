{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file I am generating the dataset using OpenAI API for ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = ''\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mountain_names.txt', 'r', encoding='utf-8') as file:\n",
    "    mountain_names = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\!Study\\Other\\DataScience_TestTask\\ner\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--dslim--distilbert-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(prompt, n, max_tokens):\n",
    "    \"\"\"\n",
    "    Function to generate sentences with name entities in square bracekts\n",
    "    through ChatGPT and process the output\n",
    "    \"\"\"\n",
    "    # Send a request to ChatGPT\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"You are a dataset generator for named entity recognition. Do not communicate with user in your answers.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        n=n,\n",
    "        temperature=1,\n",
    "        # frequency_penalty=0.02,\n",
    "        # presence_penalty=0.2\n",
    "    )\n",
    "\n",
    "    # Parse the response to get the text\n",
    "    sentences = [choice.message.content for choice in response.choices]\n",
    "    \n",
    "    # Prepare tokenized and labeled data\n",
    "    tokenized_data = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        entities = []\n",
    "        offset = 0  # Track changes in length after removing brackets\n",
    "\n",
    "        # Find all mountain names in brackets, e.g., [Mount Everest]\n",
    "        matches = re.finditer(r'\\[(.*?)\\]', sentence)\n",
    "\n",
    "        for match in matches:\n",
    "            mountain_name = match.group(1)  # Get the name without the brackets\n",
    "\n",
    "            # Tokenize the mountain name to handle multi-word names like \"Mount Everest\"\n",
    "            mountain_tokens = tokenizer.tokenize(mountain_name)\n",
    "\n",
    "            # Calculate start and end indices for the entity\n",
    "            start_idx = match.start() - offset\n",
    "            end_idx = start_idx + len(mountain_name)\n",
    "\n",
    "            # Store the entity with token positions\n",
    "            entities.append((start_idx, end_idx))\n",
    "\n",
    "            # Update the offset: 2 characters ([]) are removed\n",
    "            offset += 2\n",
    "\n",
    "        # Clean up the sentence by removing the brackets\n",
    "        cleaned_sentence = re.sub(r'\\[|\\]', '', sentence)\n",
    "\n",
    "        # Tokenize the cleaned sentence\n",
    "        tokenized_sentence = tokenizer.tokenize(cleaned_sentence)\n",
    "\n",
    "        # Initialize the labels for each token as \"O\"\n",
    "        labels = [\"O\"] * len(tokenized_sentence)\n",
    "\n",
    "        token_start_idx = 0\n",
    "        # Assign \"B-MOUNT\" and \"I-MOUNT\" labels to the corresponding tokens\n",
    "        for start_idx, end_idx in entities:\n",
    "            # Get the subword tokens for the mountain name\n",
    "            mountain_tokens = tokenizer.tokenize(cleaned_sentence[start_idx:end_idx])\n",
    "\n",
    "            # Find where the mountain name starts in the tokenized sentence\n",
    "            for i in range(token_start_idx, len(tokenized_sentence)):\n",
    "                # Look for the start of the mountain in tokenized sentence\n",
    "                if tokenized_sentence[i:i+len(mountain_tokens)] == mountain_tokens:\n",
    "                    token_start_idx = i\n",
    "                    labels[token_start_idx] = \"B-MOUNT\"\n",
    "                    for i in range(1, len(mountain_tokens)):\n",
    "                        labels[token_start_idx + i] = \"I-MOUNT\"\n",
    "                    token_start_idx += len(mountain_tokens)\n",
    "                    break\n",
    "\n",
    "        # Add the tokenized sentence and labels to the data\n",
    "        tokenized_data.append({\"tokens\": tokenized_sentence, \"labels\": labels})\n",
    "\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_prompt(mountain_names):\n",
    "    \"\"\"\n",
    "    Generate a random prompt with a few mountain names from the list.\n",
    "    \"\"\"\n",
    "    num_mountains = random.randint(1, 4)\n",
    "    selected_mountains = random.sample(mountain_names, num_mountains)\n",
    "    prompt = f\"Generate one or few sentences mentioning these mountains: {', '.join(selected_mountains)}. Enclose mountain names in square brackets.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The breath ##taking views from Mount Mitchell attract hike ##rs from all over , while the ma ##je ##stic Gross ##g ##lock ##ner stands as the highest peak in Austria . In Ukraine , the stunning scenery of Hu ##tsu ##ls ##ka Mountain offers a glimpse into the region ' s rich cultural heritage , and adventure ##rs often seek the challenge of climbing Cho ##gol ##isa , known for its striking beauty and difficult ascent .\n",
      "O O O O O B-MOUNT I-MOUNT O O O O O O O O O O O O B-MOUNT I-MOUNT I-MOUNT I-MOUNT O O O O O O O O O O O O O O O B-MOUNT I-MOUNT I-MOUNT I-MOUNT I-MOUNT O O O O O O O O O O O O O O O O O O O O O B-MOUNT I-MOUNT I-MOUNT O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = generate_random_prompt(mountain_names)\n",
    "tokenized_data = generate_sentences(prompt, 1, 200)\n",
    "print(*tokenized_data[0]['tokens'])\n",
    "print(*tokenized_data[0]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [20:15<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "num_samples = 1000\n",
    "max_tokens = 200\n",
    "sentences_per_prompt = 1\n",
    "\n",
    "for i in tqdm(range(num_samples)):\n",
    "    # Generate a random prompt\n",
    "    random_prompt = generate_random_prompt(mountain_names)\n",
    "    \n",
    "    # Generate labeled sentences for the prompt\n",
    "    tokenized_data = generate_sentences(random_prompt, n=sentences_per_prompt, max_tokens=max_tokens)\n",
    "    \n",
    "    # Add the labeled data to the full dataset\n",
    "    dataset.extend(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_dataset.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:07<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = []\n",
    "num_samples = 100\n",
    "\n",
    "for i in tqdm(range(num_samples)):\n",
    "    # Generate a random prompt\n",
    "    random_prompt = generate_random_prompt(mountain_names)\n",
    "    \n",
    "    # Generate labeled sentences for the prompt\n",
    "    tokenized_data = generate_sentences(random_prompt, n=sentences_per_prompt, max_tokens=max_tokens)\n",
    "    \n",
    "    # Add the labeled data to the full dataset\n",
    "    val_dataset.extend(tokenized_data)\n",
    "\n",
    "with open('val_dataset.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_dataset, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
